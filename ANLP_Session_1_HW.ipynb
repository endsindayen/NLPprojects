{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AhFOUj6iCWG"
   },
   "source": [
    "# Basics of Natural Language Processing (NLP)Take Home Exercise #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwEukFd8AJE7"
   },
   "source": [
    "Use the following link to find open source data sets to complete take-home exercises.\n",
    "\n",
    "[Data Sets](https://opendatascience.com/20-open-datasets-for-natural-language-processing/)\n",
    "\n",
    "Or, you can try out Assignment 1 data set for a head start to the work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxBFGZ8DadFp"
   },
   "source": [
    "# Run this code in the beginning to limit the output size of the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HBE7ck1XPX_"
   },
   "outputs": [],
   "source": [
    " from IPython.display import display, Javascript\n",
    "\n",
    "def resize_colab_cell():\n",
    "  # Change the maxHeight variable to change the max height of the output\n",
    "   display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 400})'))\n",
    "  #Change output size for the entire notebook (set to call function on cell run)\n",
    "   get_ipython().events.register('pre_run_cell', resize_colab_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jT3X0RuyndZi"
   },
   "source": [
    "### 1. Input Text\n",
    "\n",
    "Write a function to collect text data for the analysis via user input - E.g. from a text box\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ER80G_dwRkij"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maroon 5 said she will be loved and that they don't mind spending every day out on the corner in the pouring rain.\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter text data: \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI_3GXkzn5U9"
   },
   "source": [
    "### 2. Basic Analysis\n",
    "\n",
    "Perform basic text analysis on the collected text using Spacy ([spacy.io](http://spacy.io)) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "n8WSsfmTj8en"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['Maroon', 'she', 'they', 'the corner', 'the pouring rain']\n",
      "[('Maroon', 'PROPN')]\n",
      "[('5', 'NUM')]\n",
      "[('said', 'VERB')]\n",
      "[('she', 'PRON')]\n",
      "[('will', 'AUX')]\n",
      "[('be', 'AUX')]\n",
      "[('loved', 'VERB')]\n",
      "[('and', 'CCONJ')]\n",
      "[('that', 'SCONJ')]\n",
      "[('they', 'PRON')]\n",
      "[('do', 'AUX')]\n",
      "[(\"n't\", 'PART')]\n",
      "[('mind', 'VERB')]\n",
      "[('spending', 'VERB')]\n",
      "[('every', 'DET')]\n",
      "[('day', 'NOUN')]\n",
      "[('out', 'ADV')]\n",
      "[('on', 'ADP')]\n",
      "[('the', 'DET')]\n",
      "[('corner', 'NOUN')]\n",
      "[('in', 'ADP')]\n",
      "[('the', 'DET')]\n",
      "[('pouring', 'VERB')]\n",
      "[('rain', 'NOUN')]\n",
      "[('.', 'PUNCT')]\n",
      "Verbs: ['say', 'love', 'mind', 'spend', 'pour']\n",
      "(Maroon 5, every day)\n",
      "Maroon 5 (DATE)\n",
      "every day (DATE)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Nouns:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "for token in doc:\n",
    "    print([(token.text, token.pos_)])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "print(doc.ents)\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fER49EQBRZlS"
   },
   "source": [
    "### 3. Tokenizer\n",
    "Create a custom tokenizer in Python that handles:\n",
    "*   Contractions (e.g., \"don't\" → \"do n't\")\n",
    "*   Keeps punctuation as separate tokens\n",
    "*   Splits hyphenated words (e.g., \"state-of-the-art\" → \"state of the art\")\n",
    "\n",
    "Compare its results with NLTK's word_tokenize on any sample paragraph and the following examples:\n",
    "\"New York-based company\", \"It's a beautiful day!\", \"https://www.example.com\"\n",
    "\n",
    "What differences do you see? What are the advantages, and limitations of each approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY_8KMjNz6WO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQSBeMtujknV"
   },
   "source": [
    "### 4. Regex\n",
    "\n",
    "Try writing your own RegEx that can capture citations in text E.g. (Horning, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iI5xT7o8Rlcf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCuiR4kZCp32"
   },
   "source": [
    "Extract URLS following a certain format (www. or http or https:// ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V49uczwjCyd_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBIAVwtV_cvw"
   },
   "source": [
    "### 5. Word Frequency\n",
    "\n",
    "Find the list of words that occur more than 10 times in a selected corpus.\n",
    "\n",
    "Try using different forms of setup: no stopwords, custom stopwords, not removing punctuation, etc. and see what difference in results they produce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNK4qAHUFUxT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
